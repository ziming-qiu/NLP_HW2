{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import spacy\n",
    "import string\n",
    "import csv\n",
    "import io\n",
    "import argparse\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORD_LENGTH = 78\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='PyTorch CNN text classifier.')\n",
    "#parser.add_argument('--hidden_dim', type=int, default=200, metavar='N',\n",
    "#                    help='the hidden dimension of the encoder (default: 200)')\n",
    "#parser.add_argument('--ker_size', type=int, default=3, metavar='N',\n",
    "#                    help='the kernel size of the encoder (default: 3)')\n",
    "#parser.add_argument('--epochs', type=int, default=6, metavar='N',\n",
    "#                    help='number of epochs to train (default: 6)')\n",
    "#parser.add_argument('--lr', type=float, default=3e-4, metavar='LR',\n",
    "#                        help='learning rate (default: 3e-4)')\n",
    "#args = parser.parse_args()\n",
    "#print(\"training epochs: {}, learning rate: {}, hidden dimension: {}, kernel size: {}\".\n",
    "#      format(args.epochs, args.lr, args.hidden_dim, args.ker_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        #data[tokens[0]] = map(float, tokens[1:])\n",
    "        data[tokens[0]] = tokens[1:]\n",
    "    return data\n",
    "\n",
    "# entailment=0, contradict=1, neural=2\n",
    "def read_snli_tsv(path):\n",
    "    readin_data = [] \n",
    "    with open(path) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if row[2] == 'label':\n",
    "                continue\n",
    "            if row[2] == 'entailment':\n",
    "                readin_data.append((row[0],row[1], 0))\n",
    "            elif row[2] == 'contradiction':\n",
    "                readin_data.append((row[0],row[1], 1))\n",
    "            else:\n",
    "                readin_data.append((row[0],row[1], 2))\n",
    "    return readin_data\n",
    "\n",
    "data_vector = load_vectors('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    # Returns:\n",
    "    # id2char: list of chars, where id2char[i] returns char that corresponds to char i\n",
    "    # char2id: dictionary where keys represent chars and corresponding values represent indices\n",
    "    # some preprocessing\n",
    "    max_len1 = max([len(word[0]) for word in data])\n",
    "    max_len2 = max([len(word[1]) for word in data])\n",
    "    max_len = max(max_len1, max_len2)\n",
    "    all_chars = []\n",
    "    for word in data:\n",
    "        all_chars += word[0]\n",
    "        all_chars += word[1]\n",
    "    unique_chars = list(set(all_chars))\n",
    "\n",
    "    id2char = unique_chars\n",
    "    char2id = dict(zip(unique_chars, range(2,2+len(unique_chars))))\n",
    "    id2char = ['<pad>', '<unk>'] + id2char\n",
    "    char2id['<pad>'] = PAD_IDX\n",
    "    char2id['<unk>'] = UNK_IDX\n",
    "\n",
    "    return char2id, id2char, max_len\n",
    "\n",
    "def covert_to_token(data):\n",
    "    return [(tokenize(sample[0]), tokenize(sample[1]), sample[2]) for sample in data]\n",
    "\n",
    "def tokenize(sent):\n",
    "   tokens = tokenizer(sent)\n",
    "   out_tokens = []\n",
    "   for token in tokens:\n",
    "        if token.lemma_ not in punctuations:\n",
    "            if token.lemma_ in data_vector:\n",
    "                out_tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                out_tokens.append('<unk>')\n",
    "   return out_tokens\n",
    "\n",
    "### Function that preprocessed dataset\n",
    "def read_data():\n",
    "    train_data = read_snli_tsv('./hw2_data/snli_train.tsv')\n",
    "    val_data = read_snli_tsv('./hw2_data/snli_val.tsv')\n",
    "    train_data, val_data = covert_to_token(train_data), covert_to_token(val_data)\n",
    "    char2id, id2char, max_len = build_vocab(train_data)\n",
    "    return train_data, val_data, char2id, id2char, max_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, val_data, char2id, id2char, MAX_WORD_LENGTH = read_data()\n",
    "#print (\"Maximum word length of dataset is {}\".format(MAX_WORD_LENGTH))\n",
    "#print (\"Number of characters in dataset is {}\".format(len(id2char)))\n",
    "#print (\"Characters:\")\n",
    "#print (char2id.keys())\n",
    "#print (len(char2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump((train_data, val_data, char2id, id2char, MAX_WORD_LENGTH), open(\"snli_predata_save.p\", \"wb\"))\n",
    "train_data, val_data, _, _ = pickle.load(open(\"snli_data_save.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = []\n",
    "for key in data_vector:\n",
    "    all_chars.append(key)\n",
    "unique_chars = list(set(all_chars))  \n",
    "id2char = unique_chars\n",
    "char2id = dict(zip(unique_chars, range(2,2+len(unique_chars))))\n",
    "id2char = ['<pad>', '<unk>'] + id2char\n",
    "char2id['<pad>'] = PAD_IDX\n",
    "char2id['<unk>'] = UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(in_data):\n",
    "    for i in range(len(in_data)):\n",
    "        for ii in range(len(in_data[i][0])):\n",
    "            if in_data[i][0][ii] not in data_vector:\n",
    "                in_data[i][0][ii] = '<unk>'\n",
    "        for ii in range(len(in_data[i][1])):\n",
    "            if in_data[i][1][ii] not in data_vector:\n",
    "                in_data[i][1][ii] = '<unk>'\n",
    "    return in_data\n",
    "\n",
    "train_data = process_data(train_data)\n",
    "val_data = process_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros(len(id2char), 300, dtype=torch.float)\n",
    "for i in range(len(id2char)):\n",
    "    if id2char[i] == '<unk>' or id2char[i] == '<pad>':\n",
    "        continue\n",
    "    embedding_matrix[i,:] = torch.from_numpy(np.reshape(np.array(data_vector[id2char[i]],dtype=np.float32),(1,300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, char2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "        \"\"\"\n",
    "        self.data_list1, self.data_list2, self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        self.char2id = char2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        char_idx1 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.data_list1[key][:MAX_WORD_LENGTH]]\n",
    "        char_idx2 = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.data_list2[key][:MAX_WORD_LENGTH]]\n",
    "        char_idx = (char_idx1, char_idx2)\n",
    "        len_char = (len(char_idx1), len(char_idx2))\n",
    "        label = self.target_list[key]\n",
    "        return [char_idx, len_char, label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        if datum[1][0] > MAX_WORD_LENGTH:\n",
    "            length_list1.append(MAX_WORD_LENGTH)\n",
    "        else:\n",
    "            length_list1.append(datum[1][0])\n",
    "        if datum[1][1] > MAX_WORD_LENGTH:\n",
    "            length_list2.append(MAX_WORD_LENGTH)\n",
    "        else:\n",
    "            length_list2.append(datum[1][1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0][0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[1][0])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[0][1]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[1][1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        data_list1.append(padded_vec1)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    #ind_dec_order1 = np.argsort(length_list1)[::-1]\n",
    "    #ind_back_order1 = np.zeros_like(ind_dec_order1)\n",
    "    #for i in range(len(ind_dec_order1)):\n",
    "    #    ind_back_order1[ind_dec_order1[i]] = i\n",
    "    #ind_dec_order2 = np.argsort(length_list2)[::-1]\n",
    "    #ind_back_order2 = np.zeros_like(ind_dec_order2)\n",
    "    #for i in range(len(ind_dec_order2)):\n",
    "    #    ind_back_order2[ind_dec_order2[i]] = i\n",
    "    \n",
    "    \n",
    "    #data_list1 = np.array(data_list1)[ind_dec_order1]\n",
    "    #data_list2 = np.array(data_list2)[ind_dec_order2]\n",
    "    \n",
    "    #length_list1 = np.array(length_list1)[ind_dec_order1]\n",
    "    #length_list2 = np.array(length_list2)[ind_dec_order2]\n",
    "    \n",
    "    #label_list1 = np.array(label_list1)[ind_dec_order1]\n",
    "    #label_list2 = np.array(label_list2)[ind_dec_order2]\n",
    "    \n",
    "    out_data = [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(data_list2))]\n",
    "    #out_length = [torch.LongTensor(np.array(length_list1)), torch.LongTensor(np.array(length_list2))]\n",
    "    #ind_back = (ind_back_order1, ind_back_order2)\n",
    "    label_list = torch.LongTensor(np.array(label_list))\n",
    "    return (out_data, label_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(train_data, char2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_data, char2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "#test_dataset = VocabDataset(test_data, char2id)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=vocab_collate_func,\n",
    "#                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets implement basic Convolutional Neural Net model for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        #self.conv1_1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        #self.conv1_2 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size*2, int(hidden_size*0.5))\n",
    "        self.linear2 = nn.Linear(int(hidden_size*0.5), num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len1 = x[0].size()\n",
    "        batch_size, seq_len2 = x[1].size()\n",
    "\n",
    "        embed1 = embedding_matrix[x[0],:]\n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len1, hidden1.size(-1))\n",
    "        hidden1,_ = torch.max(hidden1, dim=1)\n",
    "        \n",
    "        embed2 = embedding_matrix[x[1],:]\n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len2, hidden2.size(-1))\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len2, hidden2.size(-1))\n",
    "        hidden2,_ = torch.max(hidden2, dim=1)\n",
    "        \n",
    "        hidden = torch.cat((hidden1, hidden2), 1)\n",
    "        hidden = F.relu(self.linear1(hidden))\n",
    "        logits = self.linear2(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/4], Step: [101/782], Validation Acc: 48.9\n",
      "Epoch: [1/4], Step: [101/782], Validation Acc: 48.9\n",
      "Epoch: [1/4], Step: [201/782], Validation Acc: 56.6\n",
      "Epoch: [1/4], Step: [201/782], Validation Acc: 56.6\n",
      "Epoch: [1/4], Step: [301/782], Validation Acc: 59.2\n",
      "Epoch: [1/4], Step: [301/782], Validation Acc: 59.2\n",
      "Epoch: [1/4], Step: [401/782], Validation Acc: 58.7\n",
      "Epoch: [1/4], Step: [401/782], Validation Acc: 58.7\n",
      "Epoch: [1/4], Step: [501/782], Validation Acc: 59.3\n",
      "Epoch: [1/4], Step: [501/782], Validation Acc: 59.3\n",
      "Epoch: [1/4], Step: [601/782], Validation Acc: 59.6\n",
      "Epoch: [1/4], Step: [601/782], Validation Acc: 59.6\n",
      "Epoch: [1/4], Step: [701/782], Validation Acc: 60.6\n",
      "Epoch: [1/4], Step: [701/782], Validation Acc: 60.6\n",
      "Epoch: [2/4], Step: [101/782], Validation Acc: 61.6\n",
      "Epoch: [2/4], Step: [101/782], Validation Acc: 61.6\n",
      "Epoch: [2/4], Step: [201/782], Validation Acc: 61.6\n",
      "Epoch: [2/4], Step: [201/782], Validation Acc: 61.6\n",
      "Epoch: [2/4], Step: [301/782], Validation Acc: 63.0\n",
      "Epoch: [2/4], Step: [301/782], Validation Acc: 63.0\n",
      "Epoch: [2/4], Step: [401/782], Validation Acc: 61.6\n",
      "Epoch: [2/4], Step: [401/782], Validation Acc: 61.6\n",
      "Epoch: [2/4], Step: [501/782], Validation Acc: 62.9\n",
      "Epoch: [2/4], Step: [501/782], Validation Acc: 62.9\n",
      "Epoch: [2/4], Step: [601/782], Validation Acc: 62.3\n",
      "Epoch: [2/4], Step: [601/782], Validation Acc: 62.3\n",
      "Epoch: [2/4], Step: [701/782], Validation Acc: 63.1\n",
      "Epoch: [2/4], Step: [701/782], Validation Acc: 63.1\n",
      "Epoch: [3/4], Step: [101/782], Validation Acc: 63.3\n",
      "Epoch: [3/4], Step: [101/782], Validation Acc: 63.3\n",
      "Epoch: [3/4], Step: [201/782], Validation Acc: 63.9\n",
      "Epoch: [3/4], Step: [201/782], Validation Acc: 63.9\n",
      "Epoch: [3/4], Step: [301/782], Validation Acc: 65.6\n",
      "Epoch: [3/4], Step: [301/782], Validation Acc: 65.6\n",
      "Epoch: [3/4], Step: [401/782], Validation Acc: 64.7\n",
      "Epoch: [3/4], Step: [401/782], Validation Acc: 64.7\n",
      "Epoch: [3/4], Step: [501/782], Validation Acc: 64.6\n",
      "Epoch: [3/4], Step: [501/782], Validation Acc: 64.6\n",
      "Epoch: [3/4], Step: [601/782], Validation Acc: 65.7\n",
      "Epoch: [3/4], Step: [601/782], Validation Acc: 65.7\n",
      "Epoch: [3/4], Step: [701/782], Validation Acc: 65.6\n",
      "Epoch: [3/4], Step: [701/782], Validation Acc: 65.6\n",
      "Epoch: [4/4], Step: [101/782], Validation Acc: 65.4\n",
      "Epoch: [4/4], Step: [101/782], Validation Acc: 65.4\n",
      "Epoch: [4/4], Step: [201/782], Validation Acc: 65.4\n",
      "Epoch: [4/4], Step: [201/782], Validation Acc: 65.4\n",
      "Epoch: [4/4], Step: [301/782], Validation Acc: 64.5\n",
      "Epoch: [4/4], Step: [301/782], Validation Acc: 64.5\n",
      "Epoch: [4/4], Step: [401/782], Validation Acc: 66.5\n",
      "Epoch: [4/4], Step: [401/782], Validation Acc: 66.5\n",
      "Epoch: [4/4], Step: [501/782], Validation Acc: 66.1\n",
      "Epoch: [4/4], Step: [501/782], Validation Acc: 66.1\n",
      "Epoch: [4/4], Step: [601/782], Validation Acc: 65.7\n",
      "Epoch: [4/4], Step: [601/782], Validation Acc: 65.7\n",
      "Epoch: [4/4], Step: [701/782], Validation Acc: 66.5\n",
      "Epoch: [4/4], Step: [701/782], Validation Acc: 66.5\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, labels in loader:\n",
    "        data_batch, label_batch = data, labels\n",
    "        outputs = F.softmax(model(data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=200, num_layers=2, num_classes=3, vocab_size=len(id2char))\n",
    "test_num = 0\n",
    "val_acc = []\n",
    "num_epochs = 7\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc.append(test_model(val_loader, model))\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc[test_num]))                \n",
    "            test_num += 1\n",
    "print('training acc: ',test_model(train_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training epochs: {}, learning rate: {}, hidden dimension: {}, kernel size: {}\".\n",
    "      format(args.epochs, args.lr, args.hidden_dim, args.ker_size))\n",
    "torch.save(model.state_dict(), './model/cnn_text_e{}_h{}_k{}.pth'.format(args.epochs,\n",
    "                                                                    args.hidden_dim, args.ker_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, labels in loader:\n",
    "        data_batch, label_batch = data, labels\n",
    "        outputs = F.softmax(model(data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=200, num_layers=2, num_classes=3, vocab_size=len(id2char))\n",
    "model.load_state_dict(torch.load('./model/cnn_text_e7_h200_k3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc:  67.4\n"
     ]
    }
   ],
   "source": [
    "print('validation acc: ',test_model(val_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entailment=0, contradict=1, neural=2\n",
    "def read_mnli_tsv(path):\n",
    "    telephone_data = [] \n",
    "    travel_data = [] \n",
    "    fiction_data = [] \n",
    "    government_data = [] \n",
    "    slate_data = [] \n",
    "    with open(path) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if row[2] == 'label':\n",
    "                continue\n",
    "            if row[3] == 'telephone': \n",
    "                if row[2] == 'entailment':\n",
    "                    telephone_data.append((row[0],row[1], 0))\n",
    "                elif row[2] == 'contradiction':\n",
    "                    telephone_data.append((row[0],row[1], 1))\n",
    "                else:\n",
    "                    telephone_data.append((row[0],row[1], 2))\n",
    "            if row[3] == 'travel': \n",
    "                if row[2] == 'entailment':\n",
    "                    travel_data.append((row[0],row[1], 0))\n",
    "                elif row[2] == 'contradiction':\n",
    "                    travel_data.append((row[0],row[1], 1))\n",
    "                else:\n",
    "                    travel_data.append((row[0],row[1], 2))\n",
    "            if row[3] == 'fiction': \n",
    "                if row[2] == 'entailment':\n",
    "                    fiction_data.append((row[0],row[1], 0))\n",
    "                elif row[2] == 'contradiction':\n",
    "                    fiction_data.append((row[0],row[1], 1))\n",
    "                else:\n",
    "                    fiction_data.append((row[0],row[1], 2))\n",
    "            if row[3] == 'government': \n",
    "                if row[2] == 'entailment':\n",
    "                    government_data.append((row[0],row[1], 0))\n",
    "                elif row[2] == 'contradiction':\n",
    "                    government_data.append((row[0],row[1], 1))\n",
    "                else:\n",
    "                    government_data.append((row[0],row[1], 2))\n",
    "            if row[3] == 'slate': \n",
    "                if row[2] == 'entailment':\n",
    "                    slate_data.append((row[0],row[1], 0))\n",
    "                elif row[2] == 'contradiction':\n",
    "                    slate_data.append((row[0],row[1], 1))\n",
    "                else:\n",
    "                    slate_data.append((row[0],row[1], 2))\n",
    "                    \n",
    "    return (telephone_data, travel_data, fiction_data, government_data, slate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone_data, travel_data, fiction_data, government_data, slate_data = read_mnli_tsv('./hw2_data/mnli_val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone_data = covert_to_token(telephone_data)\n",
    "travel_data = covert_to_token(travel_data)\n",
    "fiction_data = covert_to_token(fiction_data)\n",
    "government_data = covert_to_token(government_data)\n",
    "slate_data = covert_to_token(slate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone_data = process_data(telephone_data)\n",
    "travel_data = process_data(travel_data)\n",
    "fiction_data = process_data(fiction_data)\n",
    "government_data = process_data(government_data)\n",
    "slate_data = process_data(slate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone_dataset = VocabDataset(telephone_data, char2id)\n",
    "telephone_loader = torch.utils.data.DataLoader(dataset=telephone_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "travel_dataset = VocabDataset(travel_data, char2id)\n",
    "travel_loader = torch.utils.data.DataLoader(dataset=travel_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "fiction_dataset = VocabDataset(fiction_data, char2id)\n",
    "fiction_loader = torch.utils.data.DataLoader(dataset=fiction_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "government_dataset = VocabDataset(government_data, char2id)\n",
    "government_loader = torch.utils.data.DataLoader(dataset=government_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "slate_dataset = VocabDataset(slate_data, char2id)\n",
    "slate_loader = torch.utils.data.DataLoader(dataset=slate_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telephone 45.37313432835821\n",
      "travel 45.21384928716904\n",
      "fiction 42.814070351758794\n",
      "government 44.09448818897638\n",
      "slate 44.21157684630739\n"
     ]
    }
   ],
   "source": [
    "print('telephone', test_model(telephone_loader, model))\n",
    "print('travel', test_model(travel_loader, model))\n",
    "print('fiction', test_model(fiction_loader, model))\n",
    "print('government', test_model(government_loader, model))\n",
    "print('slate', test_model(slate_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
